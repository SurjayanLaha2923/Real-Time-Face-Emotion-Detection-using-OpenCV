{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a390068b-be96-439f-b5f7-ad19aed023a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical \n",
    "from keras_preprocessing.image import load_img\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D, Input\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os \n",
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf316c78-b6b0-416d-8eb3-de663e33c92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'Face Emotion Recognition/images/images/train'\n",
    "TEST_DIR = 'Face Emotion Recognition/images/images/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18e43d6c-225a-4acb-ac24-ea3d1b92ce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createdataframe(dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for label in os.listdir(dir):\n",
    "        for imagename in os.listdir(os.path.join(dir,label)):\n",
    "            image_paths.append(os.path.join(dir,label,imagename))\n",
    "            labels.append(label)\n",
    "        print(label, \"completed\")\n",
    "    return image_paths,labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0772c8a8-d31c-4e7e-9771-7e3fe764f752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry completed\n",
      "disgust completed\n",
      "fear completed\n",
      "happy completed\n",
      "neutral completed\n",
      "sad completed\n",
      "surprise completed\n"
     ]
    }
   ],
   "source": [
    "train = pd.DataFrame()\n",
    "train['image'], train['label'] = createdataframe(TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b534ff85-86c7-43ed-9e6f-3a4a4db18466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry completed\n"
     ]
    }
   ],
   "source": [
    "test = pd.DataFrame()\n",
    "test['image'], test['label'] = createdataframe(TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "511083fa-7ae8-4544-8802-8f90430b9ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   image     label\n",
      "0      Face Emotion Recognition/images/images/train\\a...     angry\n",
      "1      Face Emotion Recognition/images/images/train\\a...     angry\n",
      "2      Face Emotion Recognition/images/images/train\\a...     angry\n",
      "3      Face Emotion Recognition/images/images/train\\a...     angry\n",
      "4      Face Emotion Recognition/images/images/train\\a...     angry\n",
      "...                                                  ...       ...\n",
      "28820  Face Emotion Recognition/images/images/train\\s...  surprise\n",
      "28821  Face Emotion Recognition/images/images/train\\s...  surprise\n",
      "28822  Face Emotion Recognition/images/images/train\\s...  surprise\n",
      "28823  Face Emotion Recognition/images/images/train\\s...  surprise\n",
      "28824  Face Emotion Recognition/images/images/train\\s...  surprise\n",
      "\n",
      "[28825 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22639f52-5f14-427e-a5dd-6fe52dff4b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 image  label\n",
      "0    Face Emotion Recognition/images/images/test\\an...  angry\n",
      "1    Face Emotion Recognition/images/images/test\\an...  angry\n",
      "2    Face Emotion Recognition/images/images/test\\an...  angry\n",
      "3    Face Emotion Recognition/images/images/test\\an...  angry\n",
      "4    Face Emotion Recognition/images/images/test\\an...  angry\n",
      "..                                                 ...    ...\n",
      "467  Face Emotion Recognition/images/images/test\\an...  angry\n",
      "468  Face Emotion Recognition/images/images/test\\an...  angry\n",
      "469  Face Emotion Recognition/images/images/test\\an...  angry\n",
      "470  Face Emotion Recognition/images/images/test\\an...  angry\n",
      "471  Face Emotion Recognition/images/images/test\\an...  angry\n",
      "\n",
      "[472 rows x 2 columns]\n",
      "0      Face Emotion Recognition/images/images/test\\an...\n",
      "1      Face Emotion Recognition/images/images/test\\an...\n",
      "2      Face Emotion Recognition/images/images/test\\an...\n",
      "3      Face Emotion Recognition/images/images/test\\an...\n",
      "4      Face Emotion Recognition/images/images/test\\an...\n",
      "                             ...                        \n",
      "467    Face Emotion Recognition/images/images/test\\an...\n",
      "468    Face Emotion Recognition/images/images/test\\an...\n",
      "469    Face Emotion Recognition/images/images/test\\an...\n",
      "470    Face Emotion Recognition/images/images/test\\an...\n",
      "471    Face Emotion Recognition/images/images/test\\an...\n",
      "Name: image, Length: 472, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(test)\n",
    "print(test['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31759899-3de2-4522-8a30-4967324e051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d68c9cca-4aaf-4f3a-a1c9-79de21a7000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(images):\n",
    "    features = [] \n",
    "    for image in tqdm(images):\n",
    "        color_mode = \"gray_scale\"\n",
    "        img = load_img(image,grayscale = True)\n",
    "        img = np.array(img)\n",
    "        features.append(img)\n",
    "    features = np.array(features)\n",
    "    features = features.reshape(len(features),48,48,1)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f2021e-8624-498b-9489-a8bcdb01b2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e892329-c05c-4a44-87a6-bec0f9b1f447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7fe38a9e8e41e8a90ecce5ac90bbf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28825 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'Face Emotion Recognition/images/images/train\\\\angry\\\\.ipynb_checkpoints'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 5\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(images)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m tqdm(images):\n\u001b[0;32m      4\u001b[0m     color_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgray_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgrayscale\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(img)\n\u001b[0;32m      7\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(img)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras_preprocessing\\image\\utils.py:113\u001b[0m, in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pil_image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not import PIL.Image. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    112\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe use of `load_img` requires PIL.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 113\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    114\u001b[0m     img \u001b[38;5;241m=\u001b[39m pil_image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(f\u001b[38;5;241m.\u001b[39mread()))\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m color_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrayscale\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    116\u001b[0m         \u001b[38;5;66;03m# if image is not already an 8-bit, 16-bit or 32-bit grayscale image\u001b[39;00m\n\u001b[0;32m    117\u001b[0m         \u001b[38;5;66;03m# convert it to an 8-bit grayscale image.\u001b[39;00m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'Face Emotion Recognition/images/images/train\\\\angry\\\\.ipynb_checkpoints'"
     ]
    }
   ],
   "source": [
    "train_features = extract_features(train['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4046ff-a3a4-4076-a316-e8adce40f43d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
